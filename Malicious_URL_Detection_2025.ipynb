{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Selma266/Malicious-URL-Detection/blob/main/Malicious_URL_Detection_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAOikKsTEClu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIy2qDS7EuCG"
      },
      "source": [
        "Chargement Phishing URLs dataset + pr√©traitement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_WiYzr2Erh5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #la biblioth√®que Pandas, utilis√©e pour la manipulation et l‚Äôanalyse de donn√©es tabulaires.\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Phishing URLs_cleaned.csv\")  # read dataset url,label  #Chargement du dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuMJn3xiFSbp"
      },
      "source": [
        "Suppression des lignes redondantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfXTI-HNFUyk"
      },
      "outputs": [],
      "source": [
        "# BEFORE removing duplicates\n",
        "print(\"Before removing duplicates:\")\n",
        "print(\"Shape:\", df.shape)\n",
        "\n",
        "# Step 2: Remove duplicate rows\n",
        "df = df.drop_duplicates()   #Suppression des doublons\n",
        "\n",
        "# AFTER removing duplicates\n",
        "print(\"\\nAfter removing duplicates:\")\n",
        "print(\"Shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2QY2MMWFOZr"
      },
      "source": [
        "Supprimer les NaN+ Conversion label en valeur Numeriques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dzzNDwxFgY2"
      },
      "outputs": [],
      "source": [
        "# Supprimer les lignes o√π url ou label est NaN\n",
        "df = df.dropna(subset=[\"url\", \"label\"]).reset_index(drop=True)  #reset_index(drop=True) R√©initialise l‚Äôindex du DataFrame\n",
        "# strip() supprime les espaces au d√©but et √† la fin URL\n",
        "df[\"url\"] = df[\"url\"].str.strip()\n",
        "# Conversion des labels texte en valeurs num√©riques\n",
        "df[\"label\"] = df[\"label\"].map({\"benign\": 0, \"malicious\": 1}).astype(int) #Encodage des labels\n",
        "print(df[\"label\"].value_counts()) #V√©rification de la distribution des classes Afichages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seyw9c3TGJUM"
      },
      "source": [
        "Creation des 3 distributions dataset1(50% 50%),dataset2(90% 10%)et dataset(10%-90%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4DE265HGb9n"
      },
      "outputs": [],
      "source": [
        "#G√©n√®re les trois distributions exp√©rimentales √† partir du m√™me dataset source.\n",
        "import pandas as pd\n",
        "# S√©pare le dataset en deux sous-ensembles selon la classe (b√©nigne / malveillante).\n",
        "benign = df[df['label'] == 0]\n",
        "malicious = df[df['label'] == 1]\n",
        "#Affiche le nombre d‚Äô√©chantillons par classe pour v√©rifier la disponibilit√© des donn√©es.\n",
        "print(f\"URLs b√©nignes disponibles: {len(benign)}\")\n",
        "print(f\"URLs malveillantes disponibles: {len(malicious)}\")\n",
        "print(f\"Total URLs disponibles: {len(df)}\")\n",
        "\n",
        "# Taille fixe pour tous les datasets\\ to avoid 'Cannot take a larger sample than population error\n",
        "TOTAL_SIZE = 176883\n",
        "\n",
        "# ===== Dataset 1: √âquilibr√© (50-50) =====\n",
        "def create_balanced_dataset(benign, malicious, total_size=176883):\n",
        "    \"\"\"\n",
        "    Cr√©e un dataset √©quilibr√© avec 50% benign et 50% malicious\n",
        "    \"\"\"\n",
        "    size_per_class = total_size // 2  #  176883 // 2 = 88441 pour chaque classe\n",
        "\n",
        "    ben_sample = benign.sample(n=size_per_class, random_state=42)\n",
        "    mal_sample = malicious.sample(n=size_per_class, random_state=42)\n",
        "\n",
        "    dataset = pd.concat([ben_sample, mal_sample]) # Concat√®ne  les deux DataFrames\n",
        "    dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# ===== Dataset 2: 90% b√©nignes, 10% malveillantes =====\n",
        "def create_imbalanced_dataset_90_10(benign, malicious, total_size=176883):\n",
        "    \"\"\"\n",
        "    Cr√©e un dataset avec 90% benign et 10% malicious\n",
        "    \"\"\"\n",
        "    ben_size = int(total_size * 0.9)\n",
        "    mal_size = int(total_size * 0.1)\n",
        "\n",
        "    ben_sample = benign.sample(n=ben_size, random_state=42)\n",
        "    mal_sample = malicious.sample(n=mal_size, random_state=42)\n",
        "\n",
        "    dataset = pd.concat([ben_sample, mal_sample])\n",
        "    dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# ===== Dataset 3: 10% b√©nignes, 90% malveillantes =====\n",
        "def create_imbalanced_dataset_10_90(benign, malicious, total_size=176883):\n",
        "    \"\"\"\n",
        "    Cr√©e un dataset avec 10% benign et 90% malicious\n",
        "    \"\"\"\n",
        "    ben_size = int(total_size * 0.1)\n",
        "    mal_size = int(total_size * 0.9)\n",
        "\n",
        "    ben_sample = benign.sample(n=ben_size, random_state=42)\n",
        "    mal_sample = malicious.sample(n=mal_size, random_state=42)\n",
        "\n",
        "    dataset = pd.concat([ben_sample, mal_sample])\n",
        "    dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# ===== Cr√©er les 3 datasets =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CR√âATION DES DATASETS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dataset_balanced = create_balanced_dataset(benign, malicious, TOTAL_SIZE)\n",
        "dataset_imb_90_10 = create_imbalanced_dataset_90_10(benign, malicious, TOTAL_SIZE)\n",
        "dataset_imb_10_90 = create_imbalanced_dataset_10_90(benign, malicious, TOTAL_SIZE)\n",
        "\n",
        "# ===== Afficher les statistiques =====\n",
        "print(\"\\nüìä DATASET 1 - BALANCED (50-50):\")\n",
        "print(f\"   Total samples: {len(dataset_balanced)}\")\n",
        "print(f\"   Distribution:\")\n",
        "print(dataset_balanced['label'].value_counts().sort_index())\n",
        "print(f\"   Pourcentages:\")\n",
        "print(dataset_balanced['label'].value_counts(normalize=True).sort_index() * 100)\n",
        "\n",
        "print(\"\\nüìä DATASET 2 - IMBALANCED (90% benign - 10% malicious):\")\n",
        "print(f\"   Total samples: {len(dataset_imb_90_10)}\")\n",
        "print(f\"   Distribution:\")\n",
        "print(dataset_imb_90_10['label'].value_counts().sort_index())\n",
        "print(f\"   Pourcentages:\")\n",
        "print(dataset_imb_90_10['label'].value_counts(normalize=True).sort_index() * 100)\n",
        "\n",
        "print(\"\\nüìä DATASET 3 - IMBALANCED (10% benign - 90% malicious):\")\n",
        "print(f\"   Total samples: {len(dataset_imb_10_90)}\")\n",
        "print(f\"   Distribution:\")\n",
        "print(dataset_imb_10_90['label'].value_counts().sort_index())\n",
        "print(f\"   Pourcentages:\")\n",
        "print(dataset_imb_10_90['label'].value_counts(normalize=True).sort_index() * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbCP8QRMI75Z"
      },
      "source": [
        "Charge le tokenizer RoBERTa pr√©-entra√Æn√©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCnLEyVAf5Vm"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6qTTSBagclo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import RobertaTokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkOVrxGSfu_o"
      },
      "outputs": [],
      "source": [
        "#Charge le tokenizer RoBERTa pr√©-entra√Æn√©, qui convertit les URLs en tokens num√©riques exploitables par le mod√®le.\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")  #Ce bloc charge le tokenizer RoBERTa, mais pas le mod√®le lui-m√™me.\n",
        "#Confirme le chargement correct du tokenizer et affiche la taille de son vocabulaire.\n",
        "print(\"Tokenizer charg√© avec succ√®s!\")\n",
        "print(f\"Taille du vocabulaire: {tokenizer.vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNLC984agrUY"
      },
      "source": [
        "D√©finir la fonction de tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6aXjTr5mRV-"
      },
      "outputs": [],
      "source": [
        "# √âTAPE 2 : D√©finir la fonction de tokenisation pour tokeniser un batch de donn√©es\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"url\"],#le texte (URLs) √† convertir en tokens\n",
        "        truncation=True,  #Si le texte est trop long, on ne garde que les 128 premiers tokens\n",
        "        padding=\"max_length\",  #padding=\"max_length\" Tous les textes doivent avoir la m√™me longueur+remplit les textes courts jusqu‚Äô√† max_length\n",
        "        max_length=128 #limite les s√©quences √† 128 tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzJm7N-1nO4B"
      },
      "source": [
        " convertir les 3 dataFrames pandas en HuggingFace Dataset, RoBERTa + Trainer ne travaillent pas directement avec pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZJtdWNxn8g4"
      },
      "outputs": [],
      "source": [
        "#Conversion des datasets Pandas\n",
        "hf_balanced = Dataset.from_pandas(dataset_balanced)\n",
        "hf_90_10 = Dataset.from_pandas(dataset_imb_90_10)  #(90% benign - 10% malicious)\n",
        "hf_10_90 = Dataset.from_pandas(dataset_imb_10_90)  #(10% benign - 90% malicious)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltnDRhj2oLeg"
      },
      "source": [
        "Appliquer le tokenizer RoBERTa aux 3 datasets, Obtenir des datasets pr√™ts pour l‚Äôentra√Ænement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXiOvGcso5LD"
      },
      "outputs": [],
      "source": [
        "# Tokenisation du dataset √©quilibr√© (50‚Äì50)\n",
        "hf_balanced = hf_balanced.map(\n",
        "    tokenize_batch,#Applique la fonction tokenize_batch en batch sur le dataset Hugging Face.\n",
        "    batched=True,#Traiter les donn√©es par lots (ex: 1000 lignes √† la fois) au lieu d'une par une Taille du batch par d√©faut : 1000 lignes/ traitement vectoris√©, plus rapide pour les grands datasets.\n",
        "    remove_columns=[\"url\"]# le mod√®le n‚Äôa besoin que des input_ids et attention_mask. on garde seulement les features n√©cessaires\n",
        ")\n",
        "#Taille du batch non pr√©cis√©e ‚Üí Hugging Face utilise 1000 par d√©faut, mais peut √™tre ajust√©e si GPU limit√©."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPyqhrDdpl_A"
      },
      "outputs": [],
      "source": [
        "#Tokenisation du dataset 10‚Äì90\n",
        "hf_10_90 = hf_10_90.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    remove_columns=[\"url\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAlHOO9CpoDc"
      },
      "outputs": [],
      "source": [
        "#Tokenisation du dataset 90‚Äì10\n",
        "hf_90_10 = hf_90_10.map(\n",
        "    tokenize_batch,\n",
        "    batched=True,\n",
        "    remove_columns=[\"url\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ffqFEdop_yB"
      },
      "source": [
        "D√©finir le format PyTorch:Convertit les datasets HF en tenseurs PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSWEJS6ZqDFr"
      },
      "outputs": [],
      "source": [
        "columns = [\"input_ids\", \"attention_mask\", \"label\"] #Sp√©cifie les colonnes que le mod√®le RoBERTa utilisera pour l‚Äôentra√Ænement\n",
        "#Convertit les datasets HF en tenseurs PyTorch, pr√™ts pour l‚Äôentra√Ænement,  Ne garde que les colonnes sp√©cifi√©es (input_ids, attention_mask, label) ‚Üí optimisation m√©moire.\n",
        "hf_balanced.set_format(type=\"torch\", columns=columns)\n",
        "hf_90_10.set_format(type=\"torch\", columns=columns)\n",
        "hf_10_90.set_format(type=\"torch\", columns=columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvP5Kq7FqruR"
      },
      "source": [
        "Affichage pour faire la v√©rification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3VHxdj7pyXI"
      },
      "outputs": [],
      "source": [
        "print(hf_balanced)\n",
        "print(hf_90_10)\n",
        "print(hf_10_90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu3aHNXOq7ZV"
      },
      "source": [
        "S√©paration des donn√©es (TRAIN / TEST (80%-20%)) pour les 3 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76T6DDbbrF3S"
      },
      "outputs": [],
      "source": [
        "#S√©pare le dataset Hugging Face en train (80%) et test (20%) dataset 50%-50%\n",
        "split_balanced = hf_balanced.train_test_split(test_size=0.2, seed=42)\n",
        "train_balanced = split_balanced[\"train\"]\n",
        "test_balanced = split_balanced[\"test\"]\n",
        "\n",
        "# V√©rification affiche le nombre d‚Äô√©chantillons pour train et test afin de v√©rifier le split.\n",
        "print(\"Balanced Dataset\")\n",
        "print(\"Train size:\", len(train_balanced))\n",
        "print(\"Test size :\", len(test_balanced))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mp9rtX70SYb"
      },
      "outputs": [],
      "source": [
        "# dataset 90%-10%\n",
        "split_90_10 = hf_90_10.train_test_split(test_size=0.2, seed=42)\n",
        "train_90_10 = split_90_10[\"train\"]\n",
        "test_90_10 = split_90_10[\"test\"]\n",
        "\n",
        "# dataset 10%-90%\n",
        "split_10_90 = hf_10_90.train_test_split(test_size=0.2, seed=42)\n",
        "train_10_90 = split_10_90[\"train\"]\n",
        "test_10_90 = split_10_90[\"test\"]\n",
        "#V√©rification similaire pour chacun (taille + distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FyL_25h0qGG"
      },
      "source": [
        "FINE-TUNING AVEC LoRA\n",
        "\n",
        "Si tu entra√Ænes le mod√®le sur balanced puis tu r√©-entra√Æne sur 90/10 avec le m√™me objet model, tu ‚Äúcontamines‚Äù l‚Äôexp√©rience (carry-over).\n",
        "\n",
        "‚û°Ô∏è Tu dois recr√©er un mod√®le neuf (m√™mes poids init) pour chaque dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66MYdclZAoJh"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, TaskType #outils pour appliquer LoRA\n",
        "#Tokenizer:convertit le texte en tokens (input_ids, attention_mask) deja charger, On charge le mod√®le RoBERTa pour la classification binaire il re√ßoit ces tokens et produit des pr√©dictions/ classification\n",
        "\n",
        "def build_lora_roberta():\n",
        "# Charger mod√®le pr√©-entra√Æn√©\n",
        "    base = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2) #D√©finit num_labels=2 pour la classification binaire (benign/malicious).\n",
        "#la taille du mod√®le est grande (~125M param√®tres) ‚Üí GPU n√©cessaire pour fine-tuning complet, LoRA r√©duit ce co√ªt.\n",
        "#Param√®tres standard pour fine-tuning efficace avec LoRA (Configuration LoRA )\n",
        "    lora_config = LoraConfig(\n",
        "        r=8, #rang de la d√©composition low-rank ‚Üí contr√¥le la capacit√© d‚Äôadaptation\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"query\", \"value\"],#appliqu√© uniquement aux matrices Q et V des transformers\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_CLS # Sequence Classification, t√¢che de classification de s√©quences\n",
        "    )\n",
        "    #  Appliquer LoRA au mod√®le,Transforme le mod√®le RoBERTa en mod√®le LoRA, o√π seules certaines couches sont entra√Ænables.\n",
        "    model = get_peft_model(base, lora_config)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWH5kLeH3AnQ"
      },
      "source": [
        "Pr√©parer l‚Äôentra√Ænement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPkPYJd83Ej3"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAi3dcCk3BQj"
      },
      "outputs": [],
      "source": [
        "# TrainingArguments : classe qui permet de d√©finir tous les param√®tres d‚Äôentra√Ænement du mod√®le, Nombre d‚Äôepochs, taille des batches, learning rate, Strat√©gie d‚Äô√©valuation et de sauvegarde GPU/FP16, logging, etc.\n",
        "#Trainer : classe qui g√®re automatiquement l‚Äôentra√Ænement, l‚Äô√©valuation et la sauvegarde du mod√®le, automatisera tout l‚Äôentra√Ænement : forward/backward pass, calcul de loss, optimisation, scheduler, checkpointing.\n",
        "#Trainer appelle automatiquement  fonction compte metric √† chaque √©valuation (eval_dataset).\n",
        "#output_dir doit √™tre diff√©rent pour chaque exp√©rience,Sinon tu √©crases les r√©sultats du balanced avec ceux du 90/10 etc.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/balanced\", # dossier pour sauvegarder les checkpoints\n",
        "    num_train_epochs=1, #nombre de passages sur tout le dataset\n",
        "    per_device_train_batch_size=16, # batch size pour l'entra√Ænement\n",
        "    per_device_eval_batch_size=32,# batch size pour √©valuation\n",
        "    learning_rate=5e-5,\n",
        "    save_strategy=\"epoch\", # sauvegarder mod√®le √† chaque epoch\n",
        "    eval_strategy=\"epoch\", # Added evaluation strategy , √©valuer √† chaque fin d'epoch\n",
        "    fp16=True  # activer float16 si GPU compatible\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVL-ZFqL5xup"
      },
      "source": [
        "Fonction d‚Äô√©valuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRZ15vlB5ybH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmjCp6th51Ct"
      },
      "outputs": [],
      "source": [
        "#calculer des m√©triques personnalis√©es (precision, recall, F1-score) pendant l‚Äô√©valuation.\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Cette fonction est appel√©e par Trainer √† chaque √©valuation.\n",
        "    eval_pred: tuple (logits, labels)\n",
        "    Retourne: dictionnaire {\"precision\":..., \"recall\":..., \"f1\":...}\n",
        "    \"\"\"\n",
        "    #Entr√©e : un tuple (logits, labels), Sortie : dictionnaire avec les m√©triques, compatible avec HF Trainer.\n",
        "    logits, labels = eval_pred\n",
        "    # Convertir logits en pr√©dictions de classe\n",
        "    preds = torch.argmax(torch.tensor(logits), axis=1).numpy()\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    # Calcul des m√©triques\n",
        "    precision = precision_score(labels, preds)\n",
        "    recall = recall_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPZVz41xKWXD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  #entra√Ænement sur GPU\n",
        "#Cette fonction v√©rifie si CUDA est disponible sur ta machine. CUDA est la technologie de NVIDIA qui permet d‚Äôutiliser le GPU pour les calculs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP4n9a3MKutL"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Aucun GPU CUDA d√©tect√©\")\n",
        "\n",
        "\n",
        "\n",
        "#|# Mat√©riel | Temps attendu (1 epoch) |\n",
        "#| --------  | ----------------------- |\n",
        "#| CPU      |  20‚Äì100 h                |\n",
        "#| GPU T4   | ~30‚Äì60 min               |\n",
        "#| GPU V100 | ~20‚Äì40 min               |\n",
        "#| GPU A100 | ~5‚Äì10 min                |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppn0p8Au5_kT"
      },
      "source": [
        "Cr√©er Trainer+ Lancer l'entra√Ænement balanced dataset+calcul les metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-p_Q6gF6Oca"
      },
      "outputs": [],
      "source": [
        "\n",
        "# IMPORTANT: nouveau mod√®le pour chaque exp√©rience\n",
        "model_balanced = build_lora_roberta()\n",
        "#Cr√©er Trainer\n",
        "trainer_balanced = Trainer(\n",
        "   model= model_balanced,\n",
        "    args=training_args,\n",
        "    train_dataset=train_balanced, #  avec dataset √©quilibr√©\n",
        "    eval_dataset=test_balanced,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "# Lancer l'entra√Ænement\n",
        "trainer_balanced.train() # entra√Æne le mod√®le en appliquant LoRA sur les couches cibl√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRaxNYgT6Psh"
      },
      "outputs": [],
      "source": [
        "#√©value le mod√®le sur le dataset de test. Retourne un dictionnaire avec loss et m√©triques ( precision, recall, F1).\n",
        "metrics = trainer_balanced.evaluate()  # √©value le mod√®le sur le dataset de test\n",
        "#evaluate() est une m√©thode fournie par la classe Trainer de Hugging Face.\n",
        "#Trainer fait automatiquement : Passe le mod√®le en mode √©valuation (model.eval())\n",
        "\n",
        "#Calcule :  Validation loss (automatique), Les m√©triques d√©finies dans compute_metrics, Retourne un dictionnaire de r√©sultats\n",
        "print(\"R√©sultats finaux sur balanced dataset de test :\")\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdn9IfKs6Pd4"
      },
      "source": [
        "Lancer l'entra√Ænement 90-10 dataset+ calcul les metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nbvy6I4GBit"
      },
      "outputs": [],
      "source": [
        "#change output_dir pour chaque experience +Toujours construire un mod√®le neuf\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/90-10-dataset\", # dossier pour sauvegarder les checkpoints\n",
        "    num_train_epochs=1, #nombre de passages sur tout le dataset\n",
        "    per_device_train_batch_size=16, # batch size pour l'entra√Ænement\n",
        "    per_device_eval_batch_size=32,# batch size pour √©valuation\n",
        "    learning_rate=5e-5,\n",
        "    save_strategy=\"epoch\", # sauvegarder mod√®le √† chaque epoch\n",
        "    eval_strategy=\"epoch\", # Added evaluation strategy , √©valuer √† chaque fin d'epoch\n",
        "    fp16=True  # activer float16 si GPU compatible\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX32LGSv6f8h"
      },
      "outputs": [],
      "source": [
        "model_90_10=build_lora_roberta()\n",
        "trainer_90_10 = Trainer(\n",
        "   model= model_90_10,\n",
        "    args=training_args,\n",
        "    train_dataset=train_90_10,\n",
        "    eval_dataset=test_90_10,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_90_10.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO5vRp530sXK"
      },
      "outputs": [],
      "source": [
        "metrics = trainer_90_10.evaluate()\n",
        "print(\"R√©sultats finaux sur 90-10 dataset de test :\")\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR0MQtbB1RdT"
      },
      "source": [
        "Lancer l'entra√Ænement 10-90 dataset+ calcul les metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B48o1Um5GaNC"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/10-90-dataset\", # dossier pour sauvegarder les checkpoints\n",
        "    num_train_epochs=1, #nombre de passages sur tout le dataset\n",
        "    per_device_train_batch_size=16, # batch size pour l'entra√Ænement\n",
        "    per_device_eval_batch_size=32,# batch size pour √©valuation\n",
        "    learning_rate=5e-5,\n",
        "    save_strategy=\"epoch\", # sauvegarder mod√®le √† chaque epoch\n",
        "    eval_strategy=\"epoch\", # Added evaluation strategy , √©valuer √† chaque fin d'epoch\n",
        "    fp16=True  # activer float16 si GPU compatible\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vptqPWs66hPJ"
      },
      "outputs": [],
      "source": [
        "model_10_90=build_lora_roberta()\n",
        "trainer_10_90 = Trainer(\n",
        "    model=model_10_90,\n",
        "    args=training_args,\n",
        "    train_dataset=train_10_90,\n",
        "    eval_dataset=test_10_90,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_10_90.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO2YGVLY0-u6"
      },
      "outputs": [],
      "source": [
        "metrics = trainer_10_90.evaluate()\n",
        "print(\"R√©sultats finaux sur 10-90 dataset de test :\")\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzaB8IW962ec"
      },
      "source": [
        "Application de smote sur embeddings+ RoBERTa LORA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r55MpIm7bUtZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from datasets import Dataset\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 1 : EXTRAIRE LES EMBEDDINGS AVEC LE MOD√àLE ROBERTA\n",
        "# ========================================================================\n",
        "\n",
        "def extract_embeddings(dataset, model_name=\"roberta-base\", batch_size=32):\n",
        "    \"\"\"\n",
        "    Extrait les embeddings RoBERTa pour tout le dataset\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset HuggingFace avec input_ids, attention_mask, label\n",
        "        model_name: nom du mod√®le pour extraction d'embeddings\n",
        "        batch_size: taille des batches pour l'extraction\n",
        "\n",
        "    Returns:\n",
        "        embeddings: np.array de shape (n_samples, embedding_dim)\n",
        "        labels: np.array de shape (n_samples,)\n",
        "    \"\"\"\n",
        "    from transformers import RobertaModel\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EXTRACTION DES EMBEDDINGS ROBERTA\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Charger le mod√®le RoBERTa pour extraire les embeddings\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    roberta_model = RobertaModel.from_pretrained(model_name).to(device)\n",
        "    roberta_model.eval()\n",
        "\n",
        "    # Pr√©parer le dataloader\n",
        "    dataset_temp = dataset.remove_columns(['label'])\n",
        "    dataloader = DataLoader(dataset_temp, batch_size=batch_size)\n",
        "\n",
        "    embeddings_list = []\n",
        "\n",
        "    print(f\"üì• Extraction des embeddings en cours...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # Obtenir les embeddings (on utilise le [CLS] token)\n",
        "            outputs = roberta_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            # Prendre l'embedding du token [CLS] (premi√®re position)\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            embeddings_list.append(cls_embeddings)\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"   Trait√©: {(i + 1) * batch_size}/{len(dataset)} √©chantillons\")\n",
        "\n",
        "    # Concatener tous les embeddings\n",
        "    embeddings = np.vstack(embeddings_list)\n",
        "    labels = np.array(dataset['label'])\n",
        "\n",
        "    print(f\"‚úÖ Extraction termin√©e!\")\n",
        "    print(f\"   Shape embeddings: {embeddings.shape}\")\n",
        "    print(f\"   Shape labels: {labels.shape}\")\n",
        "\n",
        "    # Lib√©rer la m√©moire GPU\n",
        "    del roberta_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return embeddings, labels\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 2 : APPLIQUER SMOTE SUR LES EMBEDDINGS\n",
        "# ========================================================================\n",
        "\n",
        "def apply_smote_on_embeddings(embeddings, labels, random_state=42):\n",
        "    \"\"\"\n",
        "    Applique SMOTE sur les embeddings RoBERTa\n",
        "\n",
        "    Args:\n",
        "        embeddings: np.array de shape (n_samples, embedding_dim)\n",
        "        labels: np.array de shape (n_samples,)\n",
        "        random_state: seed pour reproductibilit√©\n",
        "\n",
        "    Returns:\n",
        "        embeddings_resampled: embeddings apr√®s SMOTE\n",
        "        labels_resampled: labels apr√®s SMOTE\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"APPLICATION DE SMOTE SUR LES EMBEDDINGS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Distribution avant SMOTE\n",
        "    print(f\"\\nüìä Distribution AVANT SMOTE:\")\n",
        "    counter_before = Counter(labels)\n",
        "    print(f\"   Classe 0 (benign): {counter_before[0]}\")\n",
        "    print(f\"   Classe 1 (malicious): {counter_before[1]}\")\n",
        "    print(f\"   Ratio: {counter_before[0]/counter_before[1]:.2f}:1\")\n",
        "\n",
        "    # Appliquer SMOTE\n",
        "    print(f\"\\nüîÑ Application de SMOTE sur les embeddings...\")\n",
        "    smote = SMOTE(random_state=random_state, k_neighbors=5)\n",
        "    embeddings_resampled, labels_resampled = smote.fit_resample(embeddings, labels)\n",
        "\n",
        "    # Distribution apr√®s SMOTE\n",
        "    print(f\"\\nüìä Distribution APR√àS SMOTE:\")\n",
        "    counter_after = Counter(labels_resampled)\n",
        "    print(f\"   Classe 0 (benign): {counter_after[0]}\")\n",
        "    print(f\"   Classe 1 (malicious): {counter_after[1]}\")\n",
        "    print(f\"   Ratio: {counter_after[0]/counter_after[1]:.2f}:1\")\n",
        "    print(f\"   √âchantillons ajout√©s: {len(labels_resampled) - len(labels)}\")\n",
        "\n",
        "    return embeddings_resampled, labels_resampled\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 3 : CR√âER UN CUSTOM DATASET POUR ENTRA√éNEMENT\n",
        "# ========================================================================\n",
        "\n",
        "class EmbeddingDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset personnalis√© qui utilise directement les embeddings\n",
        "    au lieu de passer par la tokenisation\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = torch.FloatTensor(embeddings)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'inputs_embeds': self.embeddings[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 4 : MOD√àLE ROBERTA PERSONNALIS√â POUR EMBEDDINGS\n",
        "# ========================================================================\n",
        "\n",
        "from transformers import RobertaForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "class RobertaForEmbeddingClassification(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper pour RoBERTa qui accepte directement les embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"roberta-base\", num_labels=2):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs_embeds, labels=None):\n",
        "        # Ajouter une dimension pour la s√©quence (batch_size, 1, hidden_size)\n",
        "        inputs_embeds = inputs_embeds.unsqueeze(1)\n",
        "\n",
        "        outputs = self.roberta.roberta(inputs_embeds=inputs_embeds)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        # Prendre le premier token pour la classification\n",
        "        logits = self.roberta.classifier(sequence_output[:, 0, :])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}\n",
        "\n",
        "def build_lora_roberta_for_embeddings():\n",
        "    \"\"\"\n",
        "    Cr√©e un mod√®le RoBERTa avec LoRA adapt√© pour les embeddings\n",
        "    \"\"\"\n",
        "    base = RobertaForEmbeddingClassification()\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_CLS\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(base, lora_config)\n",
        "    return model\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 5 : CUSTOM TRAINER POUR EMBEDDINGS\n",
        "# ========================================================================\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "class EmbeddingTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Trainer personnalis√© pour travailler avec des embeddings\n",
        "    \"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(inputs_embeds=inputs['inputs_embeds'], labels=labels)\n",
        "        loss = outputs['loss']\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 6 : PIPELINE COMPLET AVEC SMOTE SUR EMBEDDINGS\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ PIPELINE COMPLET : SMOTE SUR EMBEDDINGS + ROBERTA LORA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6.1 : BALANCED DATASET + SMOTE SUR EMBEDDINGS\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä Dataset 1/3: BALANCED (50-50) + SMOTE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extraire embeddings\n",
        "embeddings_balanced, labels_balanced = extract_embeddings(train_balanced, batch_size=32)\n",
        "\n",
        "# Appliquer SMOTE\n",
        "embeddings_balanced_smote, labels_balanced_smote = apply_smote_on_embeddings(\n",
        "    embeddings_balanced, labels_balanced, random_state=42\n",
        ")\n",
        "\n",
        "# Cr√©er les datasets\n",
        "train_emb_balanced = EmbeddingDataset(embeddings_balanced_smote, labels_balanced_smote)\n",
        "\n",
        "# Extraire embeddings pour le test (sans SMOTE)\n",
        "embeddings_test_balanced, labels_test_balanced = extract_embeddings(test_balanced, batch_size=32)\n",
        "test_emb_balanced = EmbeddingDataset(embeddings_test_balanced, labels_test_balanced)\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "print(\"\\nüèãÔ∏è Entra√Ænement du mod√®le...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/balanced_emb_smote\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n",
        "\n",
        "model_balanced_smote = build_lora_roberta_for_embeddings()\n",
        "\n",
        "trainer_balanced_smote = EmbeddingTrainer(\n",
        "    model=model_balanced_smote,\n",
        "    args=training_args,\n",
        "    train_dataset=train_emb_balanced,\n",
        "    eval_dataset=test_emb_balanced,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_balanced_smote.train()\n",
        "\n",
        "# √âvaluation\n",
        "metrics_balanced_smote = trainer_balanced_smote.evaluate()\n",
        "print(\"\\nüìä R√©sultats BALANCED + SMOTE (embeddings):\")\n",
        "print(f\"   Precision: {metrics_balanced_smote['eval_precision']:.4f}\")\n",
        "print(f\"   Recall: {metrics_balanced_smote['eval_recall']:.4f}\")\n",
        "print(f\"   F1-Score: {metrics_balanced_smote['eval_f1']:.4f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6.2 : 90-10 DATASET + SMOTE SUR EMBEDDINGS\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä Dataset 2/3: 90-10 IMBALANCED + SMOTE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extraire embeddings\n",
        "embeddings_90_10, labels_90_10 = extract_embeddings(train_90_10, batch_size=32)\n",
        "\n",
        "# Appliquer SMOTE\n",
        "embeddings_90_10_smote, labels_90_10_smote = apply_smote_on_embeddings(\n",
        "    embeddings_90_10, labels_90_10, random_state=42\n",
        ")\n",
        "\n",
        "# Cr√©er les datasets\n",
        "train_emb_90_10 = EmbeddingDataset(embeddings_90_10_smote, labels_90_10_smote)\n",
        "\n",
        "# Extraire embeddings pour le test\n",
        "embeddings_test_90_10, labels_test_90_10 = extract_embeddings(test_90_10, batch_size=32)\n",
        "test_emb_90_10 = EmbeddingDataset(embeddings_test_90_10, labels_test_90_10)\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "print(\"\\nüèãÔ∏è Entra√Ænement du mod√®le...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/90_10_emb_smote\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n",
        "\n",
        "model_90_10_smote = build_lora_roberta_for_embeddings()\n",
        "\n",
        "trainer_90_10_smote = EmbeddingTrainer(\n",
        "    model=model_90_10_smote,\n",
        "    args=training_args,\n",
        "    train_dataset=train_emb_90_10,\n",
        "    eval_dataset=test_emb_90_10,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_90_10_smote.train()\n",
        "\n",
        "# √âvaluation\n",
        "metrics_90_10_smote = trainer_90_10_smote.evaluate()\n",
        "print(\"\\nüìä R√©sultats 90-10 + SMOTE (embeddings):\")\n",
        "print(f\"   Precision: {metrics_90_10_smote['eval_precision']:.4f}\")\n",
        "print(f\"   Recall: {metrics_90_10_smote['eval_recall']:.4f}\")\n",
        "print(f\"   F1-Score: {metrics_90_10_smote['eval_f1']:.4f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6.3 : 10-90 DATASET + SMOTE SUR EMBEDDINGS\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä Dataset 3/3: 10-90 IMBALANCED + SMOTE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extraire embeddings\n",
        "embeddings_10_90, labels_10_90 = extract_embeddings(train_10_90, batch_size=32)\n",
        "\n",
        "# Appliquer SMOTE\n",
        "embeddings_10_90_smote, labels_10_90_smote = apply_smote_on_embeddings(\n",
        "    embeddings_10_90, labels_10_90, random_state=42\n",
        ")\n",
        "\n",
        "# Cr√©er les datasets\n",
        "train_emb_10_90 = EmbeddingDataset(embeddings_10_90_smote, labels_10_90_smote)\n",
        "\n",
        "# Extraire embeddings pour le test\n",
        "embeddings_test_10_90, labels_test_10_90 = extract_embeddings(test_10_90, batch_size=32)\n",
        "test_emb_10_90 = EmbeddingDataset(embeddings_test_10_90, labels_test_10_90)\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "print(\"\\nüèãÔ∏è Entra√Ænement du mod√®le...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/10_90_emb_smote\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n",
        "\n",
        "model_10_90_smote = build_lora_roberta_for_embeddings()\n",
        "\n",
        "trainer_10_90_smote = EmbeddingTrainer(\n",
        "    model=model_10_90_smote,\n",
        "    args=training_args,\n",
        "    train_dataset=train_emb_10_90,\n",
        "    eval_dataset=test_emb_10_90,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_10_90_smote.train()\n",
        "\n",
        "# √âvaluation\n",
        "metrics_10_90_smote = trainer_10_90_smote.evaluate()\n",
        "print(\"\\nüìä R√©sultats 10-90 + SMOTE (embeddings):\")\n",
        "print(f\"   Precision: {metrics_10_90_smote['eval_precision']:.4f}\")\n",
        "print(f\"   Recall: {metrics_10_90_smote['eval_recall']:.4f}\")\n",
        "print(f\"   F1-Score: {metrics_10_90_smote['eval_f1']:.4f}\")\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 7 : TABLEAU COMPARATIF FINAL\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä TABLEAU COMPARATIF COMPLET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "results_final = pd.DataFrame({\n",
        "    'Dataset': [\n",
        "        'Balanced (50-50) - Original',\n",
        "        'Balanced (50-50) - SMOTE Embeddings',\n",
        "        '90-10 - Original',\n",
        "        '90-10 - SMOTE Embeddings',\n",
        "        '10-90 - Original',\n",
        "        '10-90 - SMOTE Embeddings'\n",
        "    ],\n",
        "    'Precision': [\n",
        "        0.9990,\n",
        "        metrics_balanced_smote['eval_precision'],\n",
        "        0.9949,\n",
        "        metrics_90_10_smote['eval_precision'],\n",
        "        0.9994,\n",
        "        metrics_10_90_smote['eval_precision']\n",
        "    ],\n",
        "    'Recall': [\n",
        "        0.9864,\n",
        "        metrics_balanced_smote['eval_recall'],\n",
        "        0.9800,\n",
        "        metrics_90_10_smote['eval_recall'],\n",
        "        0.9916,\n",
        "        metrics_10_90_smote['eval_recall']\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        0.9927,\n",
        "        metrics_balanced_smote['eval_f1'],\n",
        "        0.9874,\n",
        "        metrics_90_10_smote['eval_f1'],\n",
        "        0.9955,\n",
        "        metrics_10_90_smote['eval_f1']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\")\n",
        "print(results_final.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no-9OGhxt4QT"
      },
      "outputs": [],
      "source": [
        "# ========================================================================\n",
        "# EXTENSION DU CODE : APPLICATION DE SMOTE-TOMEK SUR EMBEDDINGS + ROBERTA LORA\n",
        "# ========================================================================\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from collections import Counter\n",
        "from datasets import Dataset\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 1 : EXTRAIRE LES EMBEDDINGS AVEC LE MOD√àLE ROBERTA\n",
        "# ========================================================================\n",
        "\n",
        "def extract_embeddings(dataset, model_name=\"roberta-base\", batch_size=32):\n",
        "    \"\"\"\n",
        "    Extrait les embeddings RoBERTa pour tout le dataset\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset HuggingFace avec input_ids, attention_mask, label\n",
        "        model_name: nom du mod√®le pour extraction d'embeddings\n",
        "        batch_size: taille des batches pour l'extraction\n",
        "\n",
        "    Returns:\n",
        "        embeddings: np.array de shape (n_samples, embedding_dim)\n",
        "        labels: np.array de shape (n_samples,)\n",
        "    \"\"\"\n",
        "    from transformers import RobertaModel\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EXTRACTION DES EMBEDDINGS ROBERTA\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Charger le mod√®le RoBERTa pour extraire les embeddings\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    roberta_model = RobertaModel.from_pretrained(model_name).to(device)\n",
        "    roberta_model.eval()\n",
        "\n",
        "    # Pr√©parer le dataloader\n",
        "    dataset_temp = dataset.remove_columns(['label'])\n",
        "    dataloader = DataLoader(dataset_temp, batch_size=batch_size)\n",
        "\n",
        "    embeddings_list = []\n",
        "\n",
        "    print(f\"üì• Extraction des embeddings en cours...\")\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # Obtenir les embeddings (on utilise le [CLS] token)\n",
        "            outputs = roberta_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            # Prendre l'embedding du token [CLS] (premi√®re position)\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            embeddings_list.append(cls_embeddings)\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"   Trait√©: {(i + 1) * batch_size}/{len(dataset)} √©chantillons\")\n",
        "\n",
        "    # Concatener tous les embeddings\n",
        "    embeddings = np.vstack(embeddings_list)\n",
        "    labels = np.array(dataset['label'])\n",
        "\n",
        "    print(f\"‚úÖ Extraction termin√©e!\")\n",
        "    print(f\"   Shape embeddings: {embeddings.shape}\")\n",
        "    print(f\"   Shape labels: {labels.shape}\")\n",
        "\n",
        "    # Lib√©rer la m√©moire GPU\n",
        "    del roberta_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return embeddings, labels\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 2 : APPLIQUER SMOTE-TOMEK SUR LES EMBEDDINGS\n",
        "# ========================================================================\n",
        "\n",
        "def apply_smote_tomek_on_embeddings(embeddings, labels, random_state=42):\n",
        "    \"\"\"\n",
        "    Applique SMOTE-Tomek sur les embeddings RoBERTa\n",
        "\n",
        "    SMOTE-Tomek combine :\n",
        "    - SMOTE : Over-sampling de la classe minoritaire\n",
        "    - Tomek Links : Nettoyage des paires ambigu√´s √† la fronti√®re\n",
        "\n",
        "    Args:\n",
        "        embeddings: np.array de shape (n_samples, embedding_dim)\n",
        "        labels: np.array de shape (n_samples,)\n",
        "        random_state: seed pour reproductibilit√©\n",
        "\n",
        "    Returns:\n",
        "        embeddings_resampled: embeddings apr√®s SMOTE-Tomek\n",
        "        labels_resampled: labels apr√®s SMOTE-Tomek\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"APPLICATION DE SMOTE-TOMEK SUR LES EMBEDDINGS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Distribution avant SMOTE-Tomek\n",
        "    print(f\"\\nüìä Distribution AVANT SMOTE-Tomek:\")\n",
        "    counter_before = Counter(labels)\n",
        "    print(f\"   Classe 0 (benign): {counter_before[0]}\")\n",
        "    print(f\"   Classe 1 (malicious): {counter_before[1]}\")\n",
        "    print(f\"   Ratio: {counter_before[0]/counter_before[1]:.2f}:1\")\n",
        "    print(f\"   Total: {len(labels)} √©chantillons\")\n",
        "\n",
        "    # Appliquer SMOTE-Tomek\n",
        "    print(f\"\\nüîÑ Application de SMOTE-Tomek sur les embeddings...\")\n",
        "    print(f\"   √âtape 1: SMOTE g√©n√®re des √©chantillons synth√©tiques...\")\n",
        "    print(f\"   √âtape 2: Tomek Links nettoie les paires ambigu√´s...\")\n",
        "\n",
        "    smote_tomek = SMOTETomek(\n",
        "        random_state=random_state,\n",
        "        smote=None,  # Utilise les param√®tres par d√©faut de SMOTE\n",
        "        tomek=TomekLinks(sampling_strategy='auto')\n",
        "    )\n",
        "\n",
        "    embeddings_resampled, labels_resampled = smote_tomek.fit_resample(embeddings, labels)\n",
        "\n",
        "    # Distribution apr√®s SMOTE-Tomek\n",
        "    print(f\"\\nüìä Distribution APR√àS SMOTE-Tomek:\")\n",
        "    counter_after = Counter(labels_resampled)\n",
        "    print(f\"   Classe 0 (benign): {counter_after[0]}\")\n",
        "    print(f\"   Classe 1 (malicious): {counter_after[1]}\")\n",
        "    print(f\"   Ratio: {counter_after[0]/counter_after[1]:.2f}:1\")\n",
        "    print(f\"   Total: {len(labels_resampled)} √©chantillons\")\n",
        "\n",
        "    # Statistiques de changement\n",
        "    samples_added = len(labels_resampled) - len(labels)\n",
        "    print(f\"\\nüìà Statistiques:\")\n",
        "    print(f\"   √âchantillons SMOTE g√©n√©r√©s: {counter_after[min(counter_before, key=counter_before.get)] - counter_before[min(counter_before, key=counter_before.get)]}\")\n",
        "    print(f\"   Paires Tomek supprim√©es: {len(labels) + (counter_after[min(counter_before, key=counter_before.get)] - counter_before[min(counter_before, key=counter_before.get)]) - len(labels_resampled)}\")\n",
        "    print(f\"   Variation nette: {samples_added:+d} √©chantillons\")\n",
        "\n",
        "    return embeddings_resampled, labels_resampled\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 3 : CR√âER UN CUSTOM DATASET POUR ENTRA√éNEMENT\n",
        "# ========================================================================\n",
        "\n",
        "class EmbeddingDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Dataset personnalis√© qui utilise directement les embeddings\n",
        "    au lieu de passer par la tokenisation\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = torch.FloatTensor(embeddings)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'inputs_embeds': self.embeddings[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 4 : MOD√àLE ROBERTA PERSONNALIS√â POUR EMBEDDINGS\n",
        "# ========================================================================\n",
        "\n",
        "from transformers import RobertaForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "class RobertaForEmbeddingClassification(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper pour RoBERTa qui accepte directement les embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"roberta-base\", num_labels=2):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs_embeds, labels=None):\n",
        "        # Ajouter une dimension pour la s√©quence (batch_size, 1, hidden_size)\n",
        "        inputs_embeds = inputs_embeds.unsqueeze(1)\n",
        "\n",
        "        outputs = self.roberta.roberta(inputs_embeds=inputs_embeds)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        # Prendre le premier token pour la classification\n",
        "        logits = self.roberta.classifier(sequence_output[:, 0, :])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}\n",
        "\n",
        "def build_lora_roberta_for_embeddings():\n",
        "    \"\"\"\n",
        "    Cr√©e un mod√®le RoBERTa avec LoRA adapt√© pour les embeddings\n",
        "    \"\"\"\n",
        "    base = RobertaForEmbeddingClassification()\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_CLS\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(base, lora_config)\n",
        "    return model\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 5 : CUSTOM TRAINER POUR EMBEDDINGS\n",
        "# ========================================================================\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "class EmbeddingTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Trainer personnalis√© pour travailler avec des embeddings\n",
        "    \"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(inputs_embeds=inputs['inputs_embeds'], labels=labels)\n",
        "        loss = outputs['loss']\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 6 : PIPELINE COMPLET AVEC SMOTE-TOMEK SUR EMBEDDINGS\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ PIPELINE COMPLET : SMOTE-TOMEK SUR EMBEDDINGS + ROBERTA LORA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6.1 : BALANCED DATASET + SMOTE-TOMEK SUR EMBEDDINGS\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä Dataset 1/3: BALANCED (50-50) + SMOTE-TOMEK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extraire embeddings\n",
        "embeddings_balanced, labels_balanced = extract_embeddings(train_balanced, batch_size=32)\n",
        "\n",
        "# Appliquer SMOTE-Tomek\n",
        "embeddings_balanced_smote_tomek, labels_balanced_smote_tomek = apply_smote_tomek_on_embeddings(\n",
        "    embeddings_balanced, labels_balanced, random_state=42\n",
        ")\n",
        "\n",
        "# Cr√©er les datasets\n",
        "train_emb_balanced = EmbeddingDataset(embeddings_balanced_smote_tomek, labels_balanced_smote_tomek)\n",
        "\n",
        "# Extraire embeddings pour le test (sans SMOTE-Tomek)\n",
        "embeddings_test_balanced, labels_test_balanced = extract_embeddings(test_balanced, batch_size=32)\n",
        "test_emb_balanced = EmbeddingDataset(embeddings_test_balanced, labels_test_balanced)\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "print(\"\\nüèãÔ∏è Entra√Ænement du mod√®le...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/balanced_emb_smote_tomek\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n",
        "\n",
        "model_balanced_smote_tomek = build_lora_roberta_for_embeddings()\n",
        "\n",
        "trainer_balanced_smote_tomek = EmbeddingTrainer(\n",
        "    model=model_balanced_smote_tomek,\n",
        "    args=training_args,\n",
        "    train_dataset=train_emb_balanced,\n",
        "    eval_dataset=test_emb_balanced,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_balanced_smote_tomek.train()\n",
        "\n",
        "# √âvaluation\n",
        "metrics_balanced_smote_tomek = trainer_balanced_smote_tomek.evaluate()\n",
        "print(\"\\nüìä R√©sultats BALANCED + SMOTE-TOMEK (embeddings):\")\n",
        "print(f\"   Precision: {metrics_balanced_smote_tomek['eval_precision']:.4f}\")\n",
        "print(f\"   Recall: {metrics_balanced_smote_tomek['eval_recall']:.4f}\")\n",
        "print(f\"   F1-Score: {metrics_balanced_smote_tomek['eval_f1']:.4f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6.2 : 90-10 DATASET + SMOTE-TOMEK SUR EMBEDDINGS\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä Dataset 2/3: 90-10 IMBALANCED + SMOTE-TOMEK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extraire embeddings\n",
        "embeddings_90_10, labels_90_10 = extract_embeddings(train_90_10, batch_size=32)\n",
        "\n",
        "# Appliquer SMOTE-Tomek\n",
        "embeddings_90_10_smote_tomek, labels_90_10_smote_tomek = apply_smote_tomek_on_embeddings(\n",
        "    embeddings_90_10, labels_90_10, random_state=42\n",
        ")\n",
        "\n",
        "# Cr√©er les datasets\n",
        "train_emb_90_10 = EmbeddingDataset(embeddings_90_10_smote_tomek, labels_90_10_smote_tomek)\n",
        "\n",
        "# Extraire embeddings pour le test\n",
        "embeddings_test_90_10, labels_test_90_10 = extract_embeddings(test_90_10, batch_size=32)\n",
        "test_emb_90_10 = EmbeddingDataset(embeddings_test_90_10, labels_test_90_10)\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "print(\"\\nüèãÔ∏è Entra√Ænement du mod√®le...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/90_10_emb_smote_tomek\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n",
        "\n",
        "model_90_10_smote_tomek = build_lora_roberta_for_embeddings()\n",
        "\n",
        "trainer_90_10_smote_tomek = EmbeddingTrainer(\n",
        "    model=model_90_10_smote_tomek,\n",
        "    args=training_args,\n",
        "    train_dataset=train_emb_90_10,\n",
        "    eval_dataset=test_emb_90_10,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_90_10_smote_tomek.train()\n",
        "\n",
        "# √âvaluation\n",
        "metrics_90_10_smote_tomek = trainer_90_10_smote_tomek.evaluate()\n",
        "print(\"\\nüìä R√©sultats 90-10 + SMOTE-TOMEK (embeddings):\")\n",
        "print(f\"   Precision: {metrics_90_10_smote_tomek['eval_precision']:.4f}\")\n",
        "print(f\"   Recall: {metrics_90_10_smote_tomek['eval_recall']:.4f}\")\n",
        "print(f\"   F1-Score: {metrics_90_10_smote_tomek['eval_f1']:.4f}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6.3 : 10-90 DATASET + SMOTE-TOMEK SUR EMBEDDINGS\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä Dataset 3/3: 10-90 IMBALANCED + SMOTE-TOMEK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Extraire embeddings\n",
        "embeddings_10_90, labels_10_90 = extract_embeddings(train_10_90, batch_size=32)\n",
        "\n",
        "# Appliquer SMOTE-Tomek\n",
        "embeddings_10_90_smote_tomek, labels_10_90_smote_tomek = apply_smote_tomek_on_embeddings(\n",
        "    embeddings_10_90, labels_10_90, random_state=42\n",
        ")\n",
        "\n",
        "# Cr√©er les datasets\n",
        "train_emb_10_90 = EmbeddingDataset(embeddings_10_90_smote_tomek, labels_10_90_smote_tomek)\n",
        "\n",
        "# Extraire embeddings pour le test\n",
        "embeddings_test_10_90, labels_test_10_90 = extract_embeddings(test_10_90, batch_size=32)\n",
        "test_emb_10_90 = EmbeddingDataset(embeddings_test_10_90, labels_test_10_90)\n",
        "\n",
        "# Entra√Æner le mod√®le\n",
        "print(\"\\nüèãÔ∏è Entra√Ænement du mod√®le...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/10_90_emb_smote_tomek\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n",
        "\n",
        "model_10_90_smote_tomek = build_lora_roberta_for_embeddings()\n",
        "\n",
        "trainer_10_90_smote_tomek = EmbeddingTrainer(\n",
        "    model=model_10_90_smote_tomek,\n",
        "    args=training_args,\n",
        "    train_dataset=train_emb_10_90,\n",
        "    eval_dataset=test_emb_10_90,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_10_90_smote_tomek.train()\n",
        "\n",
        "# √âvaluation\n",
        "metrics_10_90_smote_tomek = trainer_10_90_smote_tomek.evaluate()\n",
        "print(\"\\nüìä R√©sultats 10-90 + SMOTE-TOMEK (embeddings):\")\n",
        "print(f\"   Precision: {metrics_10_90_smote_tomek['eval_precision']:.4f}\")\n",
        "print(f\"   Recall: {metrics_10_90_smote_tomek['eval_recall']:.4f}\")\n",
        "print(f\"   F1-Score: {metrics_10_90_smote_tomek['eval_f1']:.4f}\")\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 7 : TABLEAU COMPARATIF FINAL\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä TABLEAU COMPARATIF COMPLET : ORIGINAL vs SMOTE-TOMEK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "results_final = pd.DataFrame({\n",
        "    'Dataset': [\n",
        "        'Balanced (50-50) - Original',\n",
        "        'Balanced (50-50) - SMOTE-TOMEK',\n",
        "        '90-10 - Original',\n",
        "        '90-10 - SMOTE-TOMEK',\n",
        "        '10-90 - Original',\n",
        "        '10-90 - SMOTE-TOMEK'\n",
        "    ],\n",
        "    'Precision': [\n",
        "        0.9990,\n",
        "        metrics_balanced_smote_tomek['eval_precision'],\n",
        "        0.9949,\n",
        "        metrics_90_10_smote_tomek['eval_precision'],\n",
        "        0.9994,\n",
        "        metrics_10_90_smote_tomek['eval_precision']\n",
        "    ],\n",
        "    'Recall': [\n",
        "        0.9864,\n",
        "        metrics_balanced_smote_tomek['eval_recall'],\n",
        "        0.9800,\n",
        "        metrics_90_10_smote_tomek['eval_recall'],\n",
        "        0.9916,\n",
        "        metrics_10_90_smote_tomek['eval_recall']\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        0.9927,\n",
        "        metrics_balanced_smote_tomek['eval_f1'],\n",
        "        0.9874,\n",
        "        metrics_90_10_smote_tomek['eval_f1'],\n",
        "        0.9955,\n",
        "        metrics_10_90_smote_tomek['eval_f1']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\")\n",
        "print(results_final.to_string(index=False))\n",
        "\n",
        "# ========================================================================\n",
        "# √âTAPE 8 : ANALYSE DE L'IMPACT DE SMOTE-TOMEK\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìà ANALYSE DE L'IMPACT DE SMOTE-TOMEK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüîç Impact sur Balanced (50-50):\")\n",
        "delta_prec = (metrics_balanced_smote_tomek['eval_precision'] - 0.9990) * 100\n",
        "delta_rec = (metrics_balanced_smote_tomek['eval_recall'] - 0.9864) * 100\n",
        "delta_f1 = (metrics_balanced_smote_tomek['eval_f1'] - 0.9927) * 100\n",
        "print(f\"   ŒîPrecision: {delta_prec:+.2f}%\")\n",
        "print(f\"   ŒîRecall: {delta_rec:+.2f}%\")\n",
        "print(f\"   ŒîF1-Score: {delta_f1:+.2f}%\")\n",
        "\n",
        "print(\"\\nüîç Impact sur 90-10 Imbalanced:\")\n",
        "delta_prec = (metrics_90_10_smote_tomek['eval_precision'] - 0.9949) * 100\n",
        "delta_rec = (metrics_90_10_smote_tomek['eval_recall'] - 0.9800) * 100\n",
        "delta_f1 = (metrics_90_10_smote_tomek['eval_f1'] - 0.9874) * 100\n",
        "print(f\"   ŒîPrecision: {delta_prec:+.2f}%\")\n",
        "print(f\"   ŒîRecall: {delta_rec:+.2f}%\")\n",
        "print(f\"   ŒîF1-Score: {delta_f1:+.2f}%\")\n",
        "\n",
        "print(\"\\nüîç Impact sur 10-90 Imbalanced:\")\n",
        "delta_prec = (metrics_10_90_smote_tomek['eval_precision'] - 0.9994) * 100\n",
        "delta_rec = (metrics_10_90_smote_tomek['eval_recall'] - 0.9916) * 100\n",
        "delta_f1 = (metrics_10_90_smote_tomek['eval_f1'] - 0.9955) * 100\n",
        "print(f\"   ŒîPrecision: {delta_prec:+.2f}%\")\n",
        "print(f\"   ŒîRecall: {delta_rec:+.2f}%\")\n",
        "print(f\"   ŒîF1-Score: {delta_f1:+.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° OBSERVATIONS CL√âS SUR SMOTE-TOMEK:\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "SMOTE-TOMEK combine deux techniques compl√©mentaires:\n",
        "\n",
        "1Ô∏è‚É£ SMOTE (Over-sampling):\n",
        "   - G√©n√®re des √©chantillons synth√©tiques de la classe minoritaire\n",
        "   - Interpole dans l'espace d'embeddings\n",
        "   - √âquilibre la distribution des classes\n",
        "\n",
        "2Ô∏è‚É£ Tomek Links (Cleaning):\n",
        "   - Identifie et supprime les paires d'√©chantillons ambigu√´s\n",
        "   - Nettoie la fronti√®re de d√©cision\n",
        "   - Am√©liore la s√©parabilit√© des classes\n",
        "\n",
        "‚úÖ Avantages sur SMOTE seul:\n",
        "   - Meilleure qualit√© des fronti√®res de d√©cision\n",
        "   - R√©duction du bruit aux limites des classes\n",
        "   - Am√©lioration potentielle de la pr√©cision\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ PIPELINE SMOTE-TOMEK TERMIN√â AVEC SUCC√àS\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFqdgeh0nbXQ"
      },
      "outputs": [],
      "source": [
        "Votre pipeline est correct pour r√©pondre √† :\n",
        "\n",
        "‚ÄúL‚Äôoversampling par SMOTE dans l‚Äôespace embedding am√©liore-t-il les performances en contexte d√©s√©quilibr√© ?‚Äù\n",
        "Tester plusieurs ratios de classes (50/50, 90/10, 10/90)\n",
        "\n",
        "‚úîÔ∏è C‚Äôest exactement ce qu‚Äôil faut faire pour √©tudier l‚Äôimpact du d√©s√©quilibre.\n",
        "\n",
        "Vous contr√¥lez :\n",
        "\n",
        "la distribution des classes\n",
        "\n",
        "le m√™me type de donn√©es\n",
        "\n",
        "la m√™me t√¢che\n",
        "‚Üí Tr√®s bonne base exp√©rimentale.\n",
        "2) Ne jamais appliquer SMOTE sur le test\n",
        "\n",
        "‚úîÔ∏è C‚Äôest une r√®gle fondamentale respect√©e dans votre code.\n",
        "\n",
        "3) Utiliser SMOTE sur des repr√©sentations continues\n",
        "\n",
        "‚úîÔ∏è Tr√®s bon choix conceptuel.\n",
        "\n",
        "SMOTE sur input_ids ‚ùå\n",
        "\n",
        "SMOTE sur embeddings ‚úîÔ∏è\n",
        "4) Comparer avec des m√©triques adapt√©es (Precision / Recall / F1)\n",
        "\n",
        "‚úîÔ∏è Indispensable en contexte d√©s√©quilibr√©.\n",
        "\n",
        "\n",
        "Votre code : transforme vos textes en embeddings RoBERTa, r√©√©quilibre les classes avec SMOTE dans l‚Äôespace embedding, puis entra√Æne 3 mod√®les (un par distribution) et compare les performances.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-JvcCcZoc9k"
      },
      "outputs": [],
      "source": [
        "Voici, par points, ce que fait votre dernier code (SMOTE sur embeddings + RoBERTa LoRA), du d√©but √† la fin :\n",
        "\n",
        "Importe les librairies\n",
        "\n",
        "Charge PyTorch, NumPy, SMOTE (imblearn), Counter, etc.\n",
        "\n",
        "Objectif : manipuler des tenseurs, extraire des embeddings, r√©√©quilibrer des classes.\n",
        "\n",
        "D√©finit une fonction pour extraire des embeddings RoBERTa (extract_embeddings)\n",
        "\n",
        "Charge RobertaModel (l‚Äôencodeur RoBERTa).\n",
        "\n",
        "Met le mod√®le sur GPU si disponible.\n",
        "\n",
        "Parcourt le dataset par lots (DataLoader).\n",
        "\n",
        "Pour chaque batch, calcule last_hidden_state.\n",
        "\n",
        "R√©cup√®re l‚Äôembedding du premier token (token ‚ÄúCLS‚Äù / position 0) comme repr√©sentation du texte.\n",
        "\n",
        "Concat√®ne tous les embeddings en une matrice X de forme (N, hidden_size) et r√©cup√®re y (labels).\n",
        "\n",
        "Applique SMOTE sur les embeddings (apply_smote_on_embeddings)\n",
        "\n",
        "Affiche la distribution des classes avant SMOTE (Counter).\n",
        "\n",
        "Applique SMOTE dans l‚Äôespace continu des embeddings :\n",
        "\n",
        "G√©n√®re des embeddings synth√©tiques pour la classe minoritaire.\n",
        "\n",
        "R√©√©quilibre les classes (souvent vers 50/50).\n",
        "\n",
        "Affiche la distribution apr√®s SMOTE et le nombre d‚Äô√©chantillons ajout√©s.\n",
        "\n",
        "Cr√©e un Dataset PyTorch personnalis√© (EmbeddingDataset)\n",
        "\n",
        "Transforme les embeddings (float) et labels (int) en tenseurs PyTorch.\n",
        "\n",
        "Permet au Trainer de lire les donn√©es sous la forme :\n",
        "\n",
        "inputs_embeds = vecteur embedding\n",
        "\n",
        "labels = classe\n",
        "\n",
        "D√©finit un mod√®le de classification qui accepte directement des embeddings (RobertaForEmbeddingClassification)\n",
        "\n",
        "Charge RobertaForSequenceClassification.\n",
        "\n",
        "Dans forward() :\n",
        "\n",
        "Ajoute une dimension ‚Äús√©quence‚Äù (transforme (batch, hidden) en (batch, 1, hidden)).\n",
        "\n",
        "Fait passer ces embeddings via l‚Äôencodeur RoBERTa (self.roberta.roberta(inputs_embeds=...)).\n",
        "\n",
        "Prend la sortie du ‚Äúpremier token‚Äù et la passe dans la couche classifier.\n",
        "\n",
        "Calcule la loss CrossEntropy si labels est fourni.\n",
        "\n",
        "Retourne logits (et loss pendant l‚Äôentra√Ænement).\n",
        "\n",
        "Ajoute LoRA au mod√®le (build_lora_roberta_for_embeddings)\n",
        "\n",
        "Cr√©e le mod√®le pr√©c√©dent.\n",
        "\n",
        "Configure LoRA (r, alpha, dropout, modules cibl√©s query et value).\n",
        "\n",
        "‚ÄúWrap‚Äù le mod√®le avec PEFT/LoRA pour fine-tuning l√©ger (moins de param√®tres entra√Æn√©s).\n",
        "\n",
        "Cr√©e un Trainer personnalis√© (EmbeddingTrainer)\n",
        "\n",
        "Red√©finit compute_loss pour appeler correctement le mod√®le avec :\n",
        "\n",
        "inputs_embeds au lieu de input_ids\n",
        "\n",
        "R√©cup√®re la loss retourn√©e par le mod√®le, ce qui permet d‚Äôutiliser HuggingFace Trainer.\n",
        "\n",
        "Lance le pipeline complet sur 3 jeux d‚Äôentra√Ænement\n",
        "\n",
        "Pour chaque dataset (Balanced, 90/10, 10/90), le code :\n",
        "\n",
        "Extrait les embeddings du train\n",
        "\n",
        "Applique SMOTE sur embeddings du train\n",
        "\n",
        "Cr√©e EmbeddingDataset pour train\n",
        "\n",
        "Extrait les embeddings du test (sans SMOTE)\n",
        "\n",
        "Cr√©e EmbeddingDataset pour test\n",
        "\n",
        "D√©finit TrainingArguments\n",
        "\n",
        "Construit le mod√®le LoRA\n",
        "\n",
        "Entra√Æne avec EmbeddingTrainer\n",
        "\n",
        "√âvalue et affiche Precision / Recall / F1\n",
        "\n",
        "G√©n√®re un tableau comparatif final\n",
        "\n",
        "Construit un DataFrame avec :\n",
        "\n",
        "r√©sultats ‚ÄúOriginal‚Äù (valeurs fixes)\n",
        "\n",
        "r√©sultats ‚ÄúSMOTE Embeddings‚Äù (mesur√©s)\n",
        "\n",
        "Affiche le tableau pour comparer avant/apr√®s.\n",
        "\n",
        "Termine le script\n",
        "\n",
        "Affiche ‚Äúpipeline termin√©‚Äù."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKXH7oqBxnBsKjIQjfY2NY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}